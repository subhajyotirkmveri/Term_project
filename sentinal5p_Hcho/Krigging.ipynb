{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5d6ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pykrige in /home/sysadm/anaconda3/lib/python3.11/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy<2,>=1.14.5 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from pykrige) (1.24.3)\n",
      "Requirement already satisfied: scipy<2,>=1.1.0 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from pykrige) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "!pip install pykrige"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f242a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pykrige.ok import OrdinaryKriging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa71e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlags=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c299a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "import numpy as np\n",
    "\n",
    "def estimate_pollutant(df, date_column, nlags=20):\n",
    "    # Create a DataFrame with only latitude, longitude, and the specified date column\n",
    "    df1 = df[['longitude', 'latitude', date_column]]\n",
    "\n",
    "    # Create a DataFrame with latitude and longitude where the specified date column's values are NaN\n",
    "    nan_locations_df = df1[df1[date_column].isna()][['longitude', 'latitude']]\n",
    "\n",
    "    # Create a DataFrame with latitude, longitude, and the specified date column excluding NaN values\n",
    "    pollutant_df = df1.dropna(subset=[date_column])[['longitude', 'latitude', date_column]]\n",
    "\n",
    "    # Prepare the data for Ordinary Kriging\n",
    "    lons = np.array(pollutant_df['longitude'])\n",
    "    lats = np.array(pollutant_df['latitude'])\n",
    "    zdata = np.array(pollutant_df[date_column])\n",
    "\n",
    "    # Perform Ordinary Kriging\n",
    "    OK = OrdinaryKriging(lons, lats, zdata, variogram_model='linear', verbose=True, enable_plotting=False, nlags=nlags)\n",
    "    grid_lon = np.array(nan_locations_df[\"longitude\"])\n",
    "    grid_lat = np.array(nan_locations_df[\"latitude\"])\n",
    "    z1, ss1 = OK.execute('grid', grid_lon, grid_lat)\n",
    "\n",
    "    # Fill in NaN values in the specified date column with the interpolated values\n",
    "    nan_locations_df[date_column] = z1[0]\n",
    "\n",
    "    # Merge the two DataFrames based on their indices\n",
    "    merged_df = pd.concat([pollutant_df, nan_locations_df], ignore_index=False)\n",
    "\n",
    "    # Sort the DataFrame by ascending index\n",
    "    merged_df = merged_df.sort_index(ascending=True)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeee033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting data for anisotropy...\n",
      "Initializing variogram model...\n",
      "Coordinates type: 'euclidean' \n",
      "\n",
      "Using 'linear' Variogram Model\n",
      "Slope: 2.7763356633472323e-08\n",
      "Nugget: 3.7198027511863187e-08 \n",
      "\n",
      "Calculating statistics on variogram model fit...\n",
      "Executing Ordinary Kriging...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df should be the DataFrame with columns containing NaN values\n",
    "df = pd.read_csv(\"/home/sysadm/Downloads/sentinal5p_Hcho/Chosen_state_2023_sep_merge_sentinel5p_Hcho/Delhi_sep_Hcho.csv\")\n",
    "\n",
    "# Get the list of columns that have NaN values\n",
    "columns_with_nan = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "# Create an empty dictionary to store the estimated AOD DataFrames\n",
    "estimated_data = {}\n",
    "\n",
    "# Iterate through the columns with NaN values and apply estimate_aod\n",
    "for date_column in columns_with_nan:\n",
    "    estimated_data[date_column] = estimate_pollutant(df, date_column)\n",
    "\n",
    "# Create a list of columns to keep in the estimated_df\n",
    "columns_to_keep = ['longitude', 'latitude'] + columns_with_nan\n",
    "\n",
    "# Select only the desired columns from the estimated_df\n",
    "estimated_df = pd.concat([estimated_data[date_column] for date_column in columns_with_nan], axis=1)\n",
    "estimated_df = estimated_df[columns_to_keep]\n",
    "\n",
    "# Drop the duplicate 'latitude' and 'longitude' columns\n",
    "estimated_df = estimated_df.loc[:,~estimated_df.columns.duplicated()]\n",
    "\n",
    "# Create a DataFrame 'original_df' containing the original columns without NaN values\n",
    "original_columns = df.columns.difference(columns_with_nan).tolist()\n",
    "original_df = df[original_columns]\n",
    "# Merge the estimated_df and original_df based on 'latitude' and 'longitude'\n",
    "result_df = original_df.merge(estimated_df, on=['longitude', 'latitude'], how = 'inner')\n",
    "result_df.to_csv(\"/home/sysadm/Downloads/sentinal5p_Hcho/After_krigging_2023_sep_sentinal5p_Hcho/Delhi_2023_sep_O3_final.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df51e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005a641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44e9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting data for anisotropy...\n",
      "Initializing variogram model...\n",
      "Coordinates type: 'euclidean' \n",
      "\n",
      "Using 'linear' Variogram Model\n",
      "Slope: 3.951402000181847e-09\n",
      "Nugget: 4.5246303362898456e-08 \n",
      "\n",
      "Calculating statistics on variogram model fit...\n",
      "Executing Ordinary Kriging...\n",
      "\n",
      "Adjusting data for anisotropy...\n",
      "Initializing variogram model...\n",
      "Coordinates type: 'euclidean' \n",
      "\n",
      "Using 'linear' Variogram Model\n",
      "Slope: 1.5998827582284206e-09\n",
      "Nugget: 1.9373432067197393e-08 \n",
      "\n",
      "Calculating statistics on variogram model fit...\n",
      "Executing Ordinary Kriging...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# df should be the DataFrame with columns containing NaN values\n",
    "df = pd.read_csv(\"/home/sysadm/Downloads/sentinal5p_Hcho/Chosen_state_2023_sep_merge_sentinel5p_Hcho/WB_sep_Hcho.csv\")\n",
    "\n",
    "# Get the list of columns that have NaN values\n",
    "columns_with_nan = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "# Create an empty dictionary to store the estimated AOD DataFrames\n",
    "estimated_data = {}\n",
    "\n",
    "# Iterate through the columns with NaN values and apply estimate_aod\n",
    "for date_column in columns_with_nan:\n",
    "    estimated_data[date_column] = estimate_pollutant(df, date_column)\n",
    "\n",
    "# Create a list of columns to keep in the estimated_df\n",
    "columns_to_keep = ['longitude', 'latitude'] + columns_with_nan\n",
    "\n",
    "# Select only the desired columns from the estimated_df\n",
    "estimated_df = pd.concat([estimated_data[date_column] for date_column in columns_with_nan], axis=1)\n",
    "estimated_df = estimated_df[columns_to_keep]\n",
    "\n",
    "# Drop the duplicate 'latitude' and 'longitude' columns\n",
    "estimated_df = estimated_df.loc[:,~estimated_df.columns.duplicated()]\n",
    "\n",
    "# Create a DataFrame 'original_df' containing the original columns without NaN values\n",
    "original_columns = df.columns.difference(columns_with_nan).tolist()\n",
    "original_df = df[original_columns]\n",
    "# Merge the estimated_df and original_df based on 'latitude' and 'longitude'\n",
    "result_df = original_df.merge(estimated_df, on=['longitude', 'latitude'], how = 'inner')\n",
    "result_df.to_csv(\"/home/sysadm/Downloads/sentinal5p_Hcho/After_krigging_2023_sep_sentinal5p_Hcho/WB_2023_sep_O3_final.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc233d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cfe361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fce4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83541ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5553a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2169243f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
